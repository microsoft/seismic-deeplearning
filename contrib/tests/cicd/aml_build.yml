# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.

# Pull request against these branches will trigger this build
pr:
- master
- staging
- contrib

# Any commit to this branch will trigger the build.
trigger:
- master
- staging
- contrib

jobs:

# partially disable setup for now - done manually on build VM
- job: setup
  timeoutInMinutes: 10
  displayName: Setup
  pool:
    name: deepseismicagentpool
  steps:
    - bash: |
        # terminate as soon as any internal script fails
        set -e

        echo "Running setup..."
        pwd
        ls
        git branch
        uname -ra

# TODO: uncomment in the next release to bring back AML
#        # setup run environment
#        ./scripts/env_reinstall.sh
#
#        # use hardcoded root for now because not sure how env changes under ADO policy
#        DATA_ROOT="/home/alfred/data_dynamic"
#        ./tests/cicd/src/scripts/get_data_for_builds.sh ${DATA_ROOT}
#
#        # upload pre-processed data to AML build WASB storage - overwrites by default and auto-creates container name
#        azcopy --quiet --recursive \
#          --source ${DATA_ROOT}/dutch_f3/data --destination https://${BLOB_ACCOUNT_NAME}.blob.core.windows.net/${BLOB_CONTAINER_NAME}/data \
#          --dest-key ${BLOB_ACCOUNT_KEY}
#      env:
#        BLOB_ACCOUNT_NAME: $(amlbuildstore)
#        BLOB_CONTAINER_NAME: "amlbuild"
#        BLOB_ACCOUNT_KEY: $(amlbuildstorekey)
#
#
#- job: AML_pipeline_tests
#  dependsOn: setup
#  timeoutInMinutes: 20
#  displayName: AML pipeline tests
#  pool:
#    name: deepseismicagentpool
#  steps:
#  - bash: |
#      source activate seismic-interpretation
#      # TODO: add code which launches your pytest files ("pytest sometest" OR "python test.py")
#      # data is in  $(amlbuildstore).blob.core.windows.net/amlbuild/data (container amlbuild, virtual folder data)
#      # storage key is $(amlbuildstorekey)
#      az --version
#      az account show
#      az login --service-principal -u $SPIDENTITY -p $SPECRET --tenant $SPTENANT
#      az account set --subscription $SUB_ID
#      mkdir .azureml
#      cat <<EOF > .azureml/config.json
#      {
#        "subscription_id": "$SUB_ID",
#        "resource_group": "$RESOURCE_GROUP",
#        "workspace_name": "$WORKSPACE_NAME"
#      }
#      EOF
#      pytest interpretation/tests/test_train_pipeline.py || EXITCODE=123
#      exit $EXITCODE
#      pytest
#    env:
#      SUB_ID: $(subscription_id)
#      RESOURCE_GROUP: $(resource_group)
#      WORKSPACE_NAME: $(workspace_name)
#      BLOB_ACCOUNT_NAME: $(amlbuildstore)
#      BLOB_CONTAINER_NAME: "amlbuild"
#      BLOB_ACCOUNT_KEY: $(amlbuildstorekey)
#      BLOB_SUB_ID: $(subscription_id)
#      AML_COMPUTE_CLUSTER_NAME: "testcluster"
#      AML_COMPUTE_CLUSTER_MIN_NODES: "1"
#      AML_COMPUTE_CLUSTER_MAX_NODES: "8"
#      AML_COMPUTE_CLUSTER_SKU: "STANDARD_NC6"
#      SPIDENTITY: $(spidentity)
#      SPECRET: $(spsecret)
#      SPTENANT: $(sptenant)
#    displayName: 'integration tests'

# - job: AML_short_pipeline_test
#   dependsOn: setup
#   timeoutInMinutes: 5
#   displayName: AML short pipeline test
#   pool:
#     name: deepseismicagentpool
#   steps:
#   - bash: |      
#       source activate seismic-interpretation
#       # TODO: OPTIONAL! Add a job which launches entire training pipeline for 1 epoch of training (train model for single epoch)
#       # if you don't want this then delete the entire job from this file
#       python interpretation/deepseismic_interpretation/azureml_pipelines/dev/kickoff_train_pipeline.py --experiment=DEV-train-pipeline-name --orchestrator_config=orchestrator_config="interpretation/deepseismic_interpretation/azureml_pipelines/pipeline_config.json"


