{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation on F3 Netherlands dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seismic interpretation, also referred to as facies classification, is a task of determining types of rock in the earth’s subsurface, given seismic data. Seismic interpretation is used as a standard approach for determining precise locations of oil deposits for drilling, therefore reducing risks and potential losses. In recent years, there has been a great interest in using fully-supervised deep learning models for seismic interpretation. \n",
    "\n",
    "In this notebook, we demonstrate how to train a deep neural network for facies prediction using F3 Netherlands dataset. The F3 block is located in the North Sea off the shores of Netherlands. The dataset contains 6 classes (facies or lithostratigraphic units), all of which are of varying thickness (class imbalance). Processed data is available in numpy format as a `401 x 701 x 255` array. The processed F3 data is made available by [Alaudah et al. 2019](https://github.com/olivesgatech/facies_classification_benchmark).\n",
    "\n",
    "We specifically demonstrate a patch-based model approach, where we process a patch of an inline or crossline slice, instead of the entire slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To set up the conda environment and the Jupyter notebook kernel, please follow the instructions in the top-level [README.md](../../../README.md) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-specific parameters\n",
    "\n",
    "Now let's set parameters which are required only for this notebook.\n",
    "\n",
    "We use configuration files to specify experiment configuration, such as hyperparameters used in training and evaluation, as well as other experiment settings. \n",
    "\n",
    "This notebook is designed to showcase the patch-based models on Dutch F3 dataset, hence we load the configuration files from that experiment by navigating to the `experiments` folder in the root directory. Each configuration file specifies a different Computer Vision model which is loaded for this notebook.\n",
    "\n",
    "Modify the `CONFIG_FILE` variable below if you would like to run the experiment using a different configuration file from the same experiment.\n",
    "\n",
    "For \"out-of-the-box\" Docker experience we, already pre-poppulated each model configuration file with the correct paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an existing experiment configuration file\n",
    "CONFIG_FILE = (\n",
    "    \"../../../experiments/interpretation/dutchf3_patch/local/configs/hrnet.yaml\"\n",
    ")\n",
    "# number of images to score\n",
    "N_EVALUATE = 20\n",
    "# demo flag - by default notebook runs in demo mode and only fine-tunes the pre-trained model. Set to False for full re-training.\n",
    "DEMO = True\n",
    "# options are test1 or test2 - picks which Dutch F3 test set split to use\n",
    "TEST_SPLIT = \"test1\"\n",
    "\n",
    "import os\n",
    "assert os.path.isfile(CONFIG_FILE), \"Experiment config file CONFIG_FILE not found on disk\"\n",
    "assert isinstance(N_EVALUATE, int) and N_EVALUATE>0, \"Number of images to score has to be a positive integer\"\n",
    "assert isinstance(DEMO, bool), \"demo mode should be a boolean\"\n",
    "assert TEST_SPLIT == \"test1\" or TEST_SPLIT == \"test2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download and preparation\n",
    "\n",
    "To download and prepare the F3 data set, please follow the instructions in the top-level [README](../../../README.md) file. Once you have downloaded and prepared the data set, you will find your files in the following directory tree:\n",
    "\n",
    "```\n",
    "data\n",
    "├── splits\n",
    "├── test_once\n",
    "│   ├── test1_labels.npy\n",
    "│   ├── test1_seismic.npy\n",
    "│   ├── test2_labels.npy\n",
    "│   └── test2_seismic.npy\n",
    "└── train\n",
    "    ├── train_labels.npy\n",
    "    └── train_seismic.npy\n",
    "```\n",
    "\n",
    "We recommend saving the data under `$HOME/data/dutchf3` since this notebook will use that location as the data root. Otherwise, modify the `DATASET.ROOT` field in the configuration file, described next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports\n",
    "\n",
    "Let's load required libraries - the first step fixes the seeds to obtain reproducible results and the rest of the steps import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import logging.config\n",
    "from os import path\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "import yacs.config\n",
    "\n",
    "import cv2\n",
    "from albumentations import Compose, HorizontalFlip, Normalize, PadIfNeeded, Resize\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.engine import Events\n",
    "from ignite.metrics import Loss\n",
    "from ignite.utils import convert_tensor\n",
    "from toolz import compose\n",
    "from torch.utils import data\n",
    "\n",
    "from cv_lib.utils import load_log_configuration\n",
    "from cv_lib.event_handlers import SnapshotHandler, logging_handlers\n",
    "from cv_lib.event_handlers.logging_handlers import Evaluator\n",
    "from cv_lib.event_handlers import tensorboard_handlers\n",
    "from cv_lib.event_handlers.tensorboard_handlers import (\n",
    "    create_image_writer,\n",
    "    create_summary_writer,    \n",
    ")\n",
    "from cv_lib.segmentation import models\n",
    "from cv_lib.segmentation.dutchf3.engine import (\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    ")\n",
    "\n",
    "from cv_lib.segmentation.metrics import (\n",
    "    pixelwise_accuracy,\n",
    "    class_accuracy,\n",
    "    mean_class_accuracy,\n",
    "    class_iou,\n",
    "    mean_iou,\n",
    ")\n",
    "\n",
    "from cv_lib.segmentation.dutchf3.utils import (\n",
    "    current_datetime,\n",
    "    generate_path,\n",
    "    git_branch,\n",
    "    git_hash,\n",
    "    np_to_tb,\n",
    ")\n",
    "\n",
    "from deepseismic_interpretation.dutchf3.data import (\n",
    "    get_patch_loader,\n",
    "    decode_segmap,\n",
    "    get_test_loader,\n",
    ")\n",
    "\n",
    "from itkwidgets import view\n",
    "\n",
    "from utilities import (\n",
    "    plot_aline,\n",
    "    patch_label_2d,\n",
    "    compose_processing_pipeline,\n",
    "    output_processing_pipeline,\n",
    "    write_section_file,\n",
    "    runningScore,\n",
    "    validate_config_paths,\n",
    "    download_pretrained_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment configuration file\n",
    "\n",
    "Let's load the experiment configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_FILE, \"rt\") as f_read:\n",
    "    config = yacs.config.load_cfg(f_read)\n",
    "\n",
    "print(\n",
    "    f\"Configuration loaded. Please check that the DATASET.ROOT:{config.DATASET.ROOT} points to your data location.\"\n",
    ")\n",
    "print(\n",
    "    f\"To modify any of the options, please edit the configuration file {CONFIG_FILE} and reload. \\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run test pipelines to test the notebooks, which use [papermill](https://papermill.readthedocs.io/en/latest/). If this notebook is being executed as part of such pipeline, the variables below are overridden. If not, we simply update these variable from a static configuration file specified earlier.\n",
    "\n",
    "Override parameters in case we use papermill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# The number of datapoints you want to run in training or validation per batch\n",
    "# Setting to None will run whole dataset\n",
    "# useful for integration tests with a setting of something like 3\n",
    "# Use only if you want to check things are running and don't want to run\n",
    "# through whole dataset\n",
    "# The number of epochs to run in training\n",
    "max_epochs = config.TRAIN.END_EPOCH\n",
    "max_snapshots = config.TRAIN.SNAPSHOTS\n",
    "papermill = False\n",
    "dataset_root = config.DATASET.ROOT\n",
    "model_pretrained = config.MODEL.PRETRAINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back the parameters from papermill to config if papermill was used to run this notebook\n",
    "if papermill:\n",
    "    # reduce number of images scored for testing\n",
    "    N_EVALUATE=2\n",
    "\n",
    "opts = [\n",
    "    \"DATASET.ROOT\",\n",
    "    dataset_root,\n",
    "    \"TRAIN.END_EPOCH\",\n",
    "    max_epochs,\n",
    "    \"TRAIN.SNAPSHOTS\",\n",
    "    max_snapshots,\n",
    "]\n",
    "if \"PRETRAINED\" in config.MODEL.keys():\n",
    "    opts += [\"MODEL.PRETRAINED\", model_pretrained]\n",
    "\n",
    "config.merge_from_list(opts)\n",
    "\n",
    "# download pre-trained model if possible\n",
    "config = download_pretrained_model(config)\n",
    "\n",
    "# update model pretrained (in case it was changed when the pretrained model was downloaded)\n",
    "model_pretrained = config.MODEL.PRETRAINED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the final configs which are going to be used for this notebook - please check them carefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEMO:\n",
    "    opts = [\n",
    "        \"TRAIN.END_EPOCH\",\n",
    "        1,\n",
    "        \"TRAIN.SNAPSHOTS\",\n",
    "        1,\n",
    "        \"TRAIN.MAX_LR\",\n",
    "        10 ** -9,\n",
    "        \"TRAIN.MIN_LR\",\n",
    "        10 ** -9,\n",
    "    ]\n",
    "    config.merge_from_list(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds, and set CUDNN benchmark mode:\n",
    "torch.backends.cudnn.benchmark = config.CUDNN.BENCHMARK\n",
    "\n",
    "# Fix random seeds:\n",
    "torch.manual_seed(config.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(config.SEED)\n",
    "np.random.seed(seed=config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)\n",
    "validate_config_paths(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tests we reduce the number of data used by the Jupyter notebook (pending Ignite 0.3.0 where we can just reduce the number of batches per EPOCH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3 data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a few sections of the F3 data set. The processed F3 data set is stored as a 3D numpy array. Let's view slices of the data along inline and crossline directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and labels\n",
    "train_seismic = np.load(path.join(config.DATASET.ROOT, \"train/train_seismic.npy\"))\n",
    "train_labels = np.load(path.join(config.DATASET.ROOT, \"train/train_labels.npy\"))\n",
    "\n",
    "print(f\"Number of inline slices: {train_seismic.shape[0]}\")\n",
    "print(f\"Number of crossline slices: {train_seismic.shape[1]}\")\n",
    "print(f\"Depth dimension : {train_seismic.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(train_labels, slicing_planes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a __crossline__ slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "x_in = train_seismic[idx, :, :].swapaxes(0, 1)\n",
    "x_inl = train_labels[idx, :, :].swapaxes(0, 1)\n",
    "\n",
    "plot_aline(x_in, x_inl, xlabel=\"crossline (relative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot an __inline__ slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cr = train_seismic[:, idx, :].swapaxes(0, 1)\n",
    "x_crl = train_labels[:, idx, :].swapaxes(0, 1)\n",
    "\n",
    "plot_aline(x_cr, x_crl, xlabel=\"inline (relative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "load_log_configuration(config.LOG_CONFIG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.debug(config.WORKERS)\n",
    "\n",
    "scheduler_step = config.TRAIN.END_EPOCH // config.TRAIN.SNAPSHOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data augmentation\n",
    "\n",
    "Let's define our data augmentation pipeline, which includes basic transformations, such as _data normalization, resizing, and padding_ if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Augmentations\n",
    "base_aug = Compose(\n",
    "    [\n",
    "        Normalize(\n",
    "            mean=(config.TRAIN.MEAN,), std=(config.TRAIN.STD,), max_pixel_value=1\n",
    "        ),\n",
    "        PadIfNeeded(\n",
    "            min_height=config.TRAIN.PATCH_SIZE,\n",
    "            min_width=config.TRAIN.PATCH_SIZE,\n",
    "            border_mode=0,\n",
    "            always_apply=True,\n",
    "            mask_value=255,\n",
    "            value=0,\n",
    "        ),\n",
    "        Resize(\n",
    "            config.TRAIN.AUGMENTATIONS.RESIZE.HEIGHT,\n",
    "            config.TRAIN.AUGMENTATIONS.RESIZE.WIDTH,\n",
    "            always_apply=True,\n",
    "        ),\n",
    "        PadIfNeeded(\n",
    "            min_height=config.TRAIN.AUGMENTATIONS.PAD.HEIGHT,\n",
    "            min_width=config.TRAIN.AUGMENTATIONS.PAD.WIDTH,\n",
    "            border_mode=config.OPENCV_BORDER_CONSTANT,\n",
    "            always_apply=True,\n",
    "            mask_value=255,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if config.TRAIN.AUGMENTATION:\n",
    "    train_aug = Compose([base_aug, HorizontalFlip(p=0.5)])\n",
    "    val_aug = base_aug\n",
    "else:\n",
    "    raise NotImplementedError(\n",
    "        \"We don't support turning off data augmentation at this time\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, we will use a patch-based approach. Rather than using entire sections (crosslines or inlines) of the data, we extract a large number of small patches from the sections, and use the patches as our data. This allows us to generate larger set of images for training, but is also a more feasible approach for large seismic volumes. \n",
    "\n",
    "We are using a custom patch data loader from our __`deepseismic_interpretation`__ library for generating and loading patches from seismic section data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_step = config.TRAIN.END_EPOCH // config.TRAIN.SNAPSHOTS\n",
    "\n",
    "TrainPatchLoader = get_patch_loader(config)\n",
    "\n",
    "train_set = TrainPatchLoader(\n",
    "    config.DATASET.ROOT,\n",
    "    split=\"train\",\n",
    "    is_transform=True,\n",
    "    stride=config.TRAIN.STRIDE,\n",
    "    patch_size=config.TRAIN.PATCH_SIZE,\n",
    "    augmentations=train_aug,\n",
    ")\n",
    "n_classes = train_set.n_classes\n",
    "logger.info(train_set)\n",
    "val_set = TrainPatchLoader(\n",
    "    config.DATASET.ROOT,\n",
    "    split=\"val\",\n",
    "    is_transform=True,\n",
    "    stride=config.TRAIN.STRIDE,\n",
    "    patch_size=config.TRAIN.PATCH_SIZE,\n",
    "    augmentations=val_aug,\n",
    ")\n",
    "\n",
    "if papermill:\n",
    "    val_set = data.Subset(val_set, range(3))\n",
    "elif DEMO:\n",
    "    val_set = data.Subset(val_set, range(config.VALIDATION.BATCH_SIZE_PER_GPU))\n",
    "\n",
    "logger.info(val_set)\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config.TRAIN.BATCH_SIZE_PER_GPU,\n",
    "    num_workers=config.WORKERS,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=config.VALIDATION.BATCH_SIZE_PER_GPU,\n",
    "    num_workers=config.WORKERS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines the snapshot duration in batches over which we snapshot training models to disk. Variable `scheduler_step` defines how many epochs we have in a snapshot and multiplying that by the number of data points per epoch gives us the number of datapoints which we have per snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we're running in test mode, just run 2 batches\n",
    "if papermill:\n",
    "    train_len = config.TRAIN.BATCH_SIZE_PER_GPU*2    \n",
    "# if we're running in demo mode, just run 10 batches to fine-tune the model\n",
    "elif DEMO:\n",
    "    train_len = config.TRAIN.BATCH_SIZE_PER_GPU*10    \n",
    "# if we're not in test or demo modes, run the entire loop\n",
    "else:\n",
    "    train_len = len(train_loader)\n",
    "\n",
    "snapshot_duration = scheduler_step * train_len if not papermill else 2*len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also must specify a batch transformation function which allows us to selectively manipulate the data for each batch into the format which model training expects in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(batch, device=None, non_blocking=False):\n",
    "    x, y = batch\n",
    "    return (\n",
    "        convert_tensor(x, device=device, non_blocking=non_blocking),\n",
    "        convert_tensor(y, device=device, non_blocking=non_blocking),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a model to train, an optimization algorithm, and a loss function. \n",
    "\n",
    "Note that the model is loaded from our __`cv_lib`__ library, using the name of the model as specified in the configuration file. To load a different model, either change the `MODEL.NAME` field in the configuration file, or create a new one corresponding to the model you wish to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model\n",
    "model = getattr(models, config.MODEL.NAME).get_seg_model(config)\n",
    "\n",
    "# Send to GPU if available\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "model = model.to(device)\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.TRAIN.MAX_LR,\n",
    "    momentum=config.TRAIN.MOMENTUM,\n",
    "    weight_decay=config.TRAIN.WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = CosineAnnealingScheduler(\n",
    "    optimizer, \"lr\", config.TRAIN.MAX_LR, config.TRAIN.MIN_LR, cycle_size=snapshot_duration\n",
    ")\n",
    "\n",
    "# weights are inversely proportional to the frequency of the classes in the training set\n",
    "class_weights = torch.tensor(\n",
    "    config.DATASET.CLASS_WEIGHTS, device=device, requires_grad=False\n",
    ")\n",
    "\n",
    "# loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    weight=class_weights, ignore_index=255, reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We use [ignite](https://pytorch.org/ignite/index.html) framework to create training and validation loops in our codebase. Ignite provides an easy way to create compact training/validation loops without too much boilerplate code.\n",
    "\n",
    "In this notebook, we demonstrate the use of ignite on the training loop only. We create a training engine `trainer` that loops multiple times over the training dataset and updates model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training engine\n",
    "trainer = create_supervised_trainer(\n",
    "    model, optimizer, criterion, prepare_batch, device=device\n",
    ")\n",
    "\n",
    "# add learning rate scheduler\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging\n",
    "\n",
    "We add various events to the trainer, using an event system, that allows us to interact with the engine on each step of the run, such as, when the trainer is started/completed, when the epoch is started/completed and so on. \n",
    "\n",
    "Over the next few cells, we use event handlers to add the following events to the training loop:\n",
    "- log training output\n",
    "- log and schedule learning rate and\n",
    "- periodically save model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and create main output directory \n",
    "# output_dir = path.join(config.OUTPUT_DIR+\"_nb\", config.TRAIN.MODEL_DIR)\n",
    "output_dir = config.OUTPUT_DIR+\"_nb\"\n",
    "generate_path(output_dir)\n",
    "\n",
    "# define main summary writer which logs all model summaries\n",
    "summary_writer = create_summary_writer(log_dir=path.join(output_dir, config.LOG_DIR))\n",
    "\n",
    "# add logging of training output\n",
    "trainer.add_event_handler(\n",
    "    Events.ITERATION_COMPLETED,\n",
    "    logging_handlers.log_training_output(log_interval=config.TRAIN.BATCH_SIZE_PER_GPU),\n",
    ")\n",
    "\n",
    "# add logging of learning rate\n",
    "trainer.add_event_handler(Events.EPOCH_STARTED, logging_handlers.log_lr(optimizer))\n",
    "\n",
    "# log learning rate to tensorboard\n",
    "trainer.add_event_handler(\n",
    "    Events.EPOCH_STARTED,\n",
    "    tensorboard_handlers.log_lr(summary_writer, optimizer, \"epoch\"),\n",
    ")\n",
    "\n",
    "# log training summary to tensorboard as well\n",
    "trainer.add_event_handler(\n",
    "    Events.ITERATION_COMPLETED,\n",
    "    tensorboard_handlers.log_training_output(summary_writer),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also checkpoint models and snapshot them to disk with every training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add model checkpointing\n",
    "checkpoint_handler = ModelCheckpoint(\n",
    "    output_dir,\n",
    "    \"model_f3_nb\",\n",
    "    save_interval=1,\n",
    "    n_saved=1,\n",
    "    create_dir=True,\n",
    "    require_empty=False,\n",
    ")\n",
    "\n",
    "trainer.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED, checkpoint_handler, {config.MODEL.NAME: model}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to score the model on validation set as it's training. To do this we need to add helper functions to manipulate data into the required shape just as we've done to prepare each batch for training at the beginning of this notebook.\n",
    "\n",
    "We also set up evaluation metrics which we want to record on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for\n",
    "def _select_pred_and_mask(model_out_dict):\n",
    "    return (model_out_dict[\"y_pred\"].squeeze(), model_out_dict[\"mask\"].squeeze())\n",
    "\n",
    "\n",
    "def _select_max(pred_tensor):\n",
    "    return pred_tensor.max(1)[1]\n",
    "\n",
    "\n",
    "def _tensor_to_numpy(pred_tensor):\n",
    "    return pred_tensor.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "def snapshot_function():\n",
    "    return (trainer.state.iteration % snapshot_duration) == 0\n",
    "\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    prepare_batch,\n",
    "    metrics={\n",
    "        \"nll\": Loss(criterion, output_transform=_select_pred_and_mask),\n",
    "        \"pixacc\": pixelwise_accuracy(\n",
    "            n_classes, output_transform=_select_pred_and_mask, device=device\n",
    "        ),\n",
    "        \"cacc\": class_accuracy(n_classes, output_transform=_select_pred_and_mask),\n",
    "        \"mca\": mean_class_accuracy(n_classes, output_transform=_select_pred_and_mask),\n",
    "        \"ciou\": class_iou(n_classes, output_transform=_select_pred_and_mask),\n",
    "        \"mIoU\": mean_iou(n_classes, output_transform=_select_pred_and_mask),\n",
    "    },\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, Evaluator(evaluator, val_loader))\n",
    "\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    logging_handlers.log_metrics(\n",
    "        \"Validation results\",\n",
    "        metrics_dict={\n",
    "            \"nll\": \"Avg loss :\",\n",
    "            \"pixacc\": \"Pixelwise Accuracy :\",\n",
    "            \"mca\": \"Avg Class Accuracy :\",\n",
    "            \"mIoU\": \"Avg Class IoU :\",\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    tensorboard_handlers.log_metrics(\n",
    "        summary_writer,\n",
    "        trainer,\n",
    "        \"epoch\",\n",
    "        metrics_dict={\n",
    "            \"mIoU\": \"Validation/mIoU\",\n",
    "            \"nll\": \"Validation/Loss\",\n",
    "            \"mca\": \"Validation/MCA\",\n",
    "            \"pixacc\": \"Validation/Pixel_Acc\",\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "transform_func = compose(np_to_tb, decode_segmap(n_classes=n_classes), _tensor_to_numpy)\n",
    "\n",
    "transform_pred = compose(transform_func, _select_max)\n",
    "\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    create_image_writer(summary_writer, \"Validation/Image\", \"image\"),\n",
    ")\n",
    "\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    create_image_writer(\n",
    "        summary_writer, \"Validation/Mask\", \"mask\", transform_func=transform_func\n",
    "    ),\n",
    ")\n",
    "\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    create_image_writer(\n",
    "        summary_writer, \"Validation/Pred\", \"y_pred\", transform_func=transform_pred\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training engine run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(train_loader, max_epochs=config.TRAIN.END_EPOCH, epoch_length=train_len, seed = config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "Using tensorboard for monitoring runs can be quite enlightening. Just ensure that the appropriate port is open on the VM so you can access it. Below we have the command for running tensorboard in your notebook. You can as easily view it in a seperate browser window by pointing the browser to the appropriate location and port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not papermill:\n",
    "    %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not papermill:\n",
    "    %tensorboard --logdir $output_dir --port 9001 --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next evaluate the performance of the model by looking at how well it predicts facies labels on samples from the test set.\n",
    "\n",
    "We will use the following evaluation metrics:\n",
    "\n",
    "- Pixel Accuracy (PA)\n",
    "- Class Accuracy (CA)\n",
    "- Mean Class Accuracy (MCA)\n",
    "- Frequency Weighted intersection-over-union (FW IoU)\n",
    "- Mean IoU (MIoU)\n",
    "\n",
    "You have an option here to use either the pre-trained model which we provided for you or to use the model which we just fine-tuned in this notebook. By default, we use the fine-tuned model, but you can change that in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model which we just fine-tuned\n",
    "opts = [\"TEST.MODEL_PATH\", path.join(output_dir, f\"model_f3_nb_seg_hrnet_{train_len}.pth\")]\n",
    "# uncomment the line below to use the pre-trained model instead\n",
    "# opts = [\"TEST.MODEL_PATH\", config.MODEL.PRETRAINED]\n",
    "config.merge_from_list(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(config.TEST.MODEL_PATH))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the test data and define the augmentations on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "# augment entire sections with the same normalization\n",
    "section_aug = Compose(\n",
    "    [Normalize(mean=(config.TRAIN.MEAN,), std=(config.TRAIN.STD,), max_pixel_value=1,)]\n",
    ")\n",
    "\n",
    "# augment each patch and not the entire sectiom which the patches are taken from\n",
    "patch_aug = Compose(\n",
    "    [\n",
    "        Resize(\n",
    "            config.TRAIN.AUGMENTATIONS.RESIZE.HEIGHT,\n",
    "            config.TRAIN.AUGMENTATIONS.RESIZE.WIDTH,\n",
    "            always_apply=True,\n",
    "        ),\n",
    "        PadIfNeeded(\n",
    "            min_height=config.TRAIN.AUGMENTATIONS.PAD.HEIGHT,\n",
    "            min_width=config.TRAIN.AUGMENTATIONS.PAD.WIDTH,\n",
    "            border_mode=config.OPENCV_BORDER_CONSTANT,\n",
    "            always_apply=True,\n",
    "            mask_value=255,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process test data\n",
    "pre_processing = compose_processing_pipeline(config.TRAIN.DEPTH, aug=patch_aug)\n",
    "output_processing = output_processing_pipeline(config)\n",
    "\n",
    "# Select the test split\n",
    "split = TEST_SPLIT\n",
    "\n",
    "labels = np.load(path.join(config.DATASET.ROOT, \"test_once\", split + \"_labels.npy\"))\n",
    "section_file = path.join(config.DATASET.ROOT, \"splits\", \"section_\" + split + \".txt\")\n",
    "write_section_file(labels, section_file, config)\n",
    "\n",
    "# Load test data\n",
    "TestSectionLoader = get_test_loader(config)\n",
    "test_set = TestSectionLoader(\n",
    "    config.DATASET.ROOT, split=split, is_transform=True, augmentations=section_aug\n",
    ")\n",
    "# needed to fix this bug in pytorch https://github.com/pytorch/pytorch/issues/973\n",
    "# one of the workers will quit prematurely\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "test_loader = data.DataLoader(\n",
    "    test_set, batch_size=1, num_workers=config.WORKERS, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict segmentation mask on the test data\n",
    "\n",
    "For demonstration purposes and efficiency, we will only use a subset of the test data to predict segmentation mask on. More precisely, we will score `N_EVALUATE` images. If you would like to evaluate more images, set this variable to the desired number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"upper_ns\",\n",
    "    \"middle_ns\",\n",
    "    \"lower_ns\",\n",
    "    \"rijnland_chalk\",\n",
    "    \"scruff\",\n",
    "    \"zechstein\",\n",
    "]\n",
    "\n",
    "n_classes = len(CLASS_NAMES)\n",
    "\n",
    "# keep only N_EVALUATE sections to score\n",
    "test_subset = random.sample(list(test_loader), N_EVALUATE)\n",
    "\n",
    "results = list()\n",
    "running_metrics_split = runningScore(n_classes)\n",
    "\n",
    "# testing mode\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # loop over testing data\n",
    "    for i, (images, labels) in enumerate(test_subset):\n",
    "        logger.info(f\"split: {split}, section: {i}\")\n",
    "        outputs = patch_label_2d(\n",
    "            model,\n",
    "            images,\n",
    "            pre_processing,\n",
    "            output_processing,\n",
    "            config.TRAIN.PATCH_SIZE,\n",
    "            config.TEST.TEST_STRIDE,\n",
    "            config.VALIDATION.BATCH_SIZE_PER_GPU,\n",
    "            device,\n",
    "            n_classes,\n",
    "        )\n",
    "\n",
    "        pred = outputs.detach().max(1)[1].numpy()\n",
    "        gt = labels.numpy()\n",
    "        \n",
    "        # update evaluation metrics\n",
    "        running_metrics_split.update(gt, pred)\n",
    "        \n",
    "        # keep ground truth and result for plotting\n",
    "        results.append((np.squeeze(gt), np.squeeze(pred)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the obtained metrics on this subset of test images. Note that we trained our model for for a small number of epochs, for demonstration purposes, so the performance results here are not meant to be representative. \n",
    "\n",
    "The performance exceed the ones shown here when the models are trained properly. For the full report on benchmarking performance results, please refer to the [README.md](../../../README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores\n",
    "score, _ = running_metrics_split.get_scores()\n",
    "\n",
    "# Log split results\n",
    "print(f'Pixel Acc: {score[\"Pixel Acc: \"]:.3f}')\n",
    "for cdx, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f'  {class_name}_accuracy {score[\"Class Accuracy: \"][cdx]:.3f}')\n",
    "\n",
    "print(f'Mean Class Acc: {score[\"Mean Class Acc: \"]:.3f}')\n",
    "print(f'Freq Weighted IoU: {score[\"Freq Weighted IoU: \"]:.3f}')\n",
    "print(f'Mean IoU: {score[\"Mean IoU: \"]:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the predictions on entire test sections. Note that the crosslines and inlines have different dimensions, however we were able to use them jointly for our network training and evaluation, since we were using smaller patches from the sections, whose size we can control via hyperparameter in the experiment configuration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 50))\n",
    "# only plot a few images\n",
    "nplot = min(N_EVALUATE, 10)\n",
    "for idx in range(nplot):\n",
    "    # plot actual\n",
    "    plt.subplot(nplot, 2, 2 * (idx + 1) - 1)\n",
    "    plt.imshow(results[idx][0])\n",
    "    # plot predicted\n",
    "    plt.subplot(nplot, 2, 2 * (idx + 1))\n",
    "    plt.imshow(results[idx][1])\n",
    "    \n",
    "f_axes = fig.axes\n",
    "_ = f_axes[0].set_title(\"Actual\")\n",
    "_ = f_axes[1].set_title(\"Predicted\")\n",
    "fig.savefig(\"plot_predictions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "seismic-interpretation",
   "language": "python",
   "name": "seismic-interpretation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
