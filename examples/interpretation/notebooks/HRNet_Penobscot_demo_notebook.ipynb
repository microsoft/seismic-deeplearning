{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRNet training and validation on numpy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to train an HRNet model for facies prediction using [Penobscot](https://zenodo.org/record/1341774#.XepaaUB2vOg) dataset. The Penobscot 3D seismic dataset was acquired in the Scotian shelf, offshore Nova Scotia, Canada. Please refer to the top-level [README.md](../../../README.md) file to download and prepare this dataset for the experiments. \n",
    "\n",
    "The data expected in this notebook needs to be in the form of two 3D numpy arrays. One array will contain the seismic information, the other the mask. The network will be trained to take a 2D patch of data from the seismic block and learn to predict the 2D mask patch associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To set up the conda environment, please follow the instructions in the top-level [README.md](../../../README.md) file.\n",
    "\n",
    "__Note__: To register the conda environment in Jupyter, run:\n",
    "`python -m ipykernel install --user --name envname`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "from os import path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import yacs.config\n",
    "import torch\n",
    "from albumentations import Compose, HorizontalFlip, Normalize, PadIfNeeded, Resize\n",
    "from cv_lib.utils import load_log_configuration\n",
    "from cv_lib.event_handlers import (\n",
    "    SnapshotHandler,\n",
    "    logging_handlers,\n",
    "    tensorboard_handlers,\n",
    ")\n",
    "from cv_lib.event_handlers.logging_handlers import Evaluator\n",
    "from cv_lib.event_handlers.tensorboard_handlers import (\n",
    "    create_image_writer,\n",
    "    create_summary_writer,\n",
    ")\n",
    "from cv_lib.segmentation import models, extract_metric_from\n",
    "from cv_lib.segmentation.metrics import (\n",
    "    pixelwise_accuracy,\n",
    "    class_accuracy,\n",
    "    mean_class_accuracy,\n",
    "    class_iou,\n",
    "    mean_iou,\n",
    ")\n",
    "from cv_lib.segmentation.dutchf3.utils import (\n",
    "    current_datetime,\n",
    "    generate_path,\n",
    "    np_to_tb,\n",
    ")\n",
    "from cv_lib.segmentation.penobscot.engine import (\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    ")\n",
    "from deepseismic_interpretation.penobscot.data import PenobscotInlinePatchDataset\n",
    "from deepseismic_interpretation.dutchf3.data import decode_segmap\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler\n",
    "from ignite.engine import Events\n",
    "from ignite.metrics import Loss\n",
    "from ignite.utils import convert_tensor\n",
    "from toolz import compose\n",
    "from torch.utils import data\n",
    "from itkwidgets import view\n",
    "from utilities import plot_aline\n",
    "from toolz import take\n",
    "\n",
    "\n",
    "mask_value = 255\n",
    "_SEG_COLOURS = np.asarray(\n",
    "    [[241, 238, 246], [208, 209, 230], [166, 189, 219], [116, 169, 207], [54, 144, 192], [5, 112, 176], [3, 78, 123]]\n",
    ")\n",
    "\n",
    "# experiment configuration file\n",
    "CONFIG_FILE = \"./configs/hrnet.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_batch(batch, device=None, non_blocking=False):\n",
    "    x, y, ids, patch_locations = batch\n",
    "    return (\n",
    "        convert_tensor(x, device=device, non_blocking=non_blocking),\n",
    "        convert_tensor(y, device=device, non_blocking=non_blocking),\n",
    "        ids,\n",
    "        patch_locations,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment configuration file\n",
    "We use configuration files to specify experiment configuration, such as hyperparameters used in training and evaluation, as well as other experiment settings. We provide several configuration files for this notebook, under `./configs`, mainly differing in the DNN architecture used for defining the model.\n",
    "\n",
    "Modify the `CONFIG_FILE` variable above if you would like to run the experiment using a different configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_FILE, \"rt\") as f_read:\n",
    "    config = yacs.config.load_cfg(f_read)\n",
    "\n",
    "print(f'Configuration loaded. Please check that the DATASET.ROOT:{config.DATASET.ROOT} points to your data location.')\n",
    "print(f'To modify any of the options, please edit the configuration file {CONFIG_FILE} and reload. \\n')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# The number of datapoints you want to run in training or validation per batch \n",
    "# Setting to None will run whole dataset\n",
    "# useful for integration tests with a setting of something like 3\n",
    "# Use only if you want to check things are running and don't want to run\n",
    "# through whole dataset\n",
    "max_iterations = None  \n",
    "# The number of epochs to run in training\n",
    "max_epochs = config.TRAIN.END_EPOCH  \n",
    "max_snapshots = config.TRAIN.SNAPSHOTS\n",
    "dataset_root = config.DATASET.ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from toolz import pipe\n",
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = os.path.join(dataset_root, \"inlines\")\n",
    "mask_dir = os.path.join(dataset_root, \"masks\")\n",
    "\n",
    "image_iter = pipe(os.path.join(image_dir, \"*.tiff\"), glob.iglob,)\n",
    "\n",
    "_open_to_array = compose(np.array, Image.open)\n",
    "\n",
    "\n",
    "def open_image_mask(image_path):\n",
    "    return pipe(image_path, _open_to_array)\n",
    "\n",
    "\n",
    "def _mask_filename(imagepath):\n",
    "    file_part = os.path.splitext(os.path.split(imagepath)[-1].strip())[0]\n",
    "    return os.path.join(mask_dir, file_part + \"_mask.png\")\n",
    "\n",
    "\n",
    "image_list = sorted(list(image_iter))\n",
    "image_list_array = [_open_to_array(i) for i in image_list]\n",
    "mask_list_array = [pipe(i, _mask_filename, _open_to_array) for i in image_list]\n",
    "mask = np.stack(mask_list_array, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(mask, slicing_planes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view slices of the data along inline and crossline directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "x_in = image_list_array[idx]\n",
    "x_inl = mask_list_array[idx]\n",
    "\n",
    "plot_aline(x_in, x_inl, xlabel=\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "load_log_configuration(config.LOG_CONFIG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.debug(config.WORKERS)\n",
    "scheduler_step = max_epochs // max_snapshots\n",
    "torch.backends.cudnn.benchmark = config.CUDNN.BENCHMARK\n",
    "\n",
    "torch.manual_seed(config.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(config.SEED)\n",
    "np.random.seed(seed=config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data augmentation\n",
    "\n",
    "Let's define our data augmentation pipeline, which includes basic transformations, such as _data normalization, resizing, and padding_ if necessary.\n",
    "The padding is carried out twice becuase if we split the inline or crossline slice into multiple patches then some of these patches will be at the edge of the slice and may not contain a full patch worth of data. To compensate to this and have same size patches in the batch (a requirement) we need to pad them.\n",
    "So our basic augmentation is:\n",
    "- Normalize\n",
    "- Pad if needed to initial size\n",
    "- Resize to a larger size\n",
    "- Pad further if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Augmentations\n",
    "basic_aug = Compose(\n",
    "    [\n",
    "        Normalize(mean=(config.TRAIN.MEAN,), std=(config.TRAIN.STD,), max_pixel_value=config.TRAIN.MAX,),\n",
    "        PadIfNeeded(\n",
    "            min_height=config.TRAIN.PATCH_SIZE,\n",
    "            min_width=config.TRAIN.PATCH_SIZE,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            always_apply=True,\n",
    "            mask_value=mask_value,\n",
    "            value=0,\n",
    "        ),\n",
    "        Resize(config.TRAIN.AUGMENTATIONS.RESIZE.HEIGHT, config.TRAIN.AUGMENTATIONS.RESIZE.WIDTH, always_apply=True,),\n",
    "        PadIfNeeded(\n",
    "            min_height=config.TRAIN.AUGMENTATIONS.PAD.HEIGHT,\n",
    "            min_width=config.TRAIN.AUGMENTATIONS.PAD.WIDTH,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            always_apply=True,\n",
    "            mask_value=mask_value,\n",
    "            value=0,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "if config.TRAIN.AUGMENTATION:\n",
    "    train_aug = Compose([basic_aug, HorizontalFlip(p=0.5)])\n",
    "    val_aug = basic_aug\n",
    "else:\n",
    "    train_aug = val_aug = basic_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, we will use a patch-based approach. Rather than using entire sections (crosslines or inlines) of the data, we extract a large number of small patches from the sections, and use the patches as our data. This allows us to generate larger set of images for training, but is also a more feasible approach for large seismic volumes.\n",
    "\n",
    "We are using a custom patch data loader from our __`deepseismic_interpretation`__ library for generating and loading patches from seismic section data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "train_set = PenobscotInlinePatchDataset(\n",
    "    dataset_root,\n",
    "    config.TRAIN.PATCH_SIZE,\n",
    "    config.TRAIN.STRIDE,\n",
    "    split=\"train\",\n",
    "    transforms=train_aug,\n",
    "    n_channels=config.MODEL.IN_CHANNELS,\n",
    "    complete_patches_only=config.TRAIN.COMPLETE_PATCHES_ONLY,\n",
    ")\n",
    "\n",
    "val_set = PenobscotInlinePatchDataset(\n",
    "    dataset_root,\n",
    "    config.TRAIN.PATCH_SIZE,\n",
    "    config.TRAIN.STRIDE,\n",
    "    split=\"val\",\n",
    "    transforms=val_aug,\n",
    "    n_channels=config.MODEL.IN_CHANNELS,\n",
    "    complete_patches_only=config.VALIDATION.COMPLETE_PATCHES_ONLY,\n",
    ")\n",
    "\n",
    "logger.info(train_set)\n",
    "logger.info(val_set)\n",
    "\n",
    "n_classes = train_set.n_classes\n",
    "train_loader = data.DataLoader(\n",
    "    train_set, batch_size=config.TRAIN.BATCH_SIZE_PER_GPU, num_workers=config.WORKERS, shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(val_set, batch_size=config.VALIDATION.BATCH_SIZE_PER_GPU, num_workers=config.WORKERS,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model training\n",
    "Next, let's define a model to train, an optimization algorithm, and a loss function.\n",
    "\n",
    "Note that the model is loaded from our __`cv_lib`__ library, using the name of the model as specified in the configuration file. To load a different model, either change the `MODEL.NAME` field in the configuration file, or create a new one corresponding to the model you wish to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(models, config.MODEL.NAME).get_seg_model(config)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "model = model.to(device)  # Send to GPU\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=config.TRAIN.MAX_LR, momentum=config.TRAIN.MOMENTUM, weight_decay=config.TRAIN.WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "output_dir = generate_path(config.OUTPUT_DIR, config.MODEL.NAME, current_datetime(),)\n",
    "summary_writer = create_summary_writer(log_dir=path.join(output_dir, config.LOG_DIR))\n",
    "snapshot_duration = scheduler_step * len(train_loader)\n",
    "scheduler = CosineAnnealingScheduler(optimizer, \"lr\", config.TRAIN.MAX_LR, config.TRAIN.MIN_LR, snapshot_duration)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=mask_value, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We use [ignite](https://pytorch.org/ignite/index.html) framework to create training and validation loops in our codebase. Ignite provides an easy way to create compact training/validation loops without too much boilerplate code.\n",
    "\n",
    "In this notebook, we demonstrate the use of ignite on the training loop only. We create a training engine `trainer` that loops multiple times over the training dataset and updates model parameters. In addition, we add various events to the trainer, using an event system, that allows us to interact with the engine on each step of the run, such as, when the trainer is started/completed, when the epoch is started/completed and so on.\n",
    "\n",
    "In the cell below, we use event handlers to add the following events to the training loop:\n",
    "- log training output\n",
    "- log and schedule learning rate and\n",
    "- periodically save model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(model, optimizer, criterion, _prepare_batch, device=device)\n",
    "\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "trainer.add_event_handler(\n",
    "    Events.ITERATION_COMPLETED, logging_handlers.log_training_output(log_interval=config.PRINT_FREQ),\n",
    ")\n",
    "trainer.add_event_handler(Events.EPOCH_STARTED, logging_handlers.log_lr(optimizer))\n",
    "trainer.add_event_handler(\n",
    "    Events.EPOCH_STARTED, tensorboard_handlers.log_lr(summary_writer, optimizer, \"epoch\"),\n",
    ")\n",
    "trainer.add_event_handler(\n",
    "    Events.ITERATION_COMPLETED, tensorboard_handlers.log_training_output(summary_writer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_pred_and_mask(model_out_dict):\n",
    "    return (model_out_dict[\"y_pred\"].squeeze(), model_out_dict[\"mask\"].squeeze())\n",
    "\n",
    "\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    _prepare_batch,\n",
    "    metrics={\n",
    "        \"pixacc\": pixelwise_accuracy(n_classes, output_transform=_select_pred_and_mask),\n",
    "        \"nll\": Loss(criterion, output_transform=_select_pred_and_mask),\n",
    "        \"cacc\": class_accuracy(n_classes, output_transform=_select_pred_and_mask),\n",
    "        \"mca\": mean_class_accuracy(n_classes, output_transform=_select_pred_and_mask),\n",
    "        \"ciou\": class_iou(n_classes, output_transform=_select_pred_and_mask),\n",
    "        \"mIoU\": mean_iou(n_classes, output_transform=_select_pred_and_mask),\n",
    "    },\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "if max_iterations is not None:\n",
    "    val_loader = take(max_iterations, val_loader)\n",
    "\n",
    "# Set the validation run to start on the epoch completion of the training run\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, Evaluator(evaluator, val_loader))\n",
    "\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    logging_handlers.log_metrics(\n",
    "        \"Validation results\",\n",
    "        metrics_dict={\n",
    "            \"nll\": \"Avg loss :\",\n",
    "            \"pixacc\": \"Pixelwise Accuracy :\",\n",
    "            \"mca\": \"Avg Class Accuracy :\",\n",
    "            \"mIoU\": \"Avg Class IoU :\",\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    tensorboard_handlers.log_metrics(\n",
    "        summary_writer,\n",
    "        trainer,\n",
    "        \"epoch\",\n",
    "        metrics_dict={\n",
    "            \"mIoU\": \"Validation/mIoU\",\n",
    "            \"nll\": \"Validation/Loss\",\n",
    "            \"mca\": \"Validation/MCA\",\n",
    "            \"pixacc\": \"Validation/Pixel_Acc\",\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def _select_max(pred_tensor):\n",
    "    return pred_tensor.max(1)[1]\n",
    "\n",
    "\n",
    "def _tensor_to_numpy(pred_tensor):\n",
    "    return pred_tensor.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "transform_func = compose(np_to_tb, decode_segmap(n_classes=n_classes, label_colours=_SEG_COLOURS), _tensor_to_numpy,)\n",
    "\n",
    "transform_pred = compose(transform_func, _select_max)\n",
    "\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED, create_image_writer(summary_writer, \"Validation/Image\", \"image\"),\n",
    ")\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    create_image_writer(summary_writer, \"Validation/Mask\", \"mask\", transform_func=transform_func),\n",
    ")\n",
    "evaluator.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    create_image_writer(summary_writer, \"Validation/Pred\", \"y_pred\", transform_func=transform_pred),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "Below we define the function that will save the best performing models based on mean IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snapshot_function():\n",
    "    return (trainer.state.iteration % snapshot_duration) == 0\n",
    "\n",
    "\n",
    "checkpoint_handler = SnapshotHandler(\n",
    "    path.join(output_dir, config.TRAIN.MODEL_DIR), config.MODEL.NAME, extract_metric_from(\"mIoU\"), snapshot_function,\n",
    ")\n",
    "evaluator.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {\"model\": model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training engine run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_iterations is not None:\n",
    "    train_loader = take(max_iterations, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting training\")\n",
    "trainer.run(train_loader, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "Using tensorboard for monitoring runs can be quite enlightening. Just ensure that the appropriate port is open on the VM so you can access it. Below we have the command for running tensorboard in your notebook. You can as easily view it in a seperate browser window by pointing the browser to the appropriate location and port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_epochs>1:\n",
    "    %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_epochs>1:\n",
    "    %tensorboard --logdir outputs --port 6007 --host 0.0.0.0"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "seismic-interpretation",
   "language": "python",
   "name": "seismic-interpretation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
